# -*- coding: utf-8 -*-
"""BRACIS_2024.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M4kGBtchs28KWyM-MHBheReX8By7Kocr

The following code illustrates the methodology adopted in this study to evaluate the performance of Large Language Models (LLMs). We used a comprehensive set of metrics to analyze the effectiveness and consistency of the GPT-4 and Gemini models in solving complex questions from the 2024 admission exam of the Technological Institute of Aeronautics (ITA). The metrics include Accuracy, Precision, Response Consistency, Consistent Errors, and the integrated MAILLM metric.
"""

!apt-get install texlive-latex-extra texlive-fonts-recommended dvipng cm-super

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Data
data = {
    'Question': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],
    'Correct Answer': ['B', 'E', 'A', 'B', 'D', 'C', 'E', 'D', 'C', 'D', 'A', 'C'],
    'GPT-4_turbo_Test_1': ['B', 'C', 'A', 'B', 'B', 'C', 'E', 'D', 'A', 'B', 'A', 'E'],
    'GPT-4_turbo_Test_2': ['B', 'C', 'C', 'B', 'C', 'C', 'B', 'D', 'D', 'A', 'C', 'E'],
    'GPT-4_turbo_Test_3': ['B', 'D', 'A', 'A', 'B', 'C', 'E', 'D', 'B', 'A', 'C', 'D'],
    'Gemini_Advanced_Test_1': ['D', 'C', 'A', 'A', 'D', 'C', 'E', 'A', 'nan', 'D', 'A', 'A'],
    'Gemini_Advanced_Test_2': ['B', 'C', 'A', 'B', 'nan', 'C', 'nan', 'A', 'nan', 'D', 'C', 'A'],
    'Gemini_Advanced_Test_3': ['B', 'C', 'A', 'B', 'C', 'C', 'B', 'B', 'nan', 'E', 'A', 'E']
}

# Create a DataFrame
df = pd.DataFrame(data)

df



# Use Computer Modern font
plt.rc('font', family='serif', serif='cmr10')
#plt.rc('text', usetex=True)

# Function to calculate proportion of correct and incorrect answers
def calculate_proportions(df, correct_col, test_cols):
    proportions = []
    for col in test_cols:
        correct = (df[correct_col] == df[col]).sum()
        incorrect = len(df) - correct  # Treating NaNs as incorrect
        proportions.append([correct, incorrect])
    return np.array(proportions).T

# Test columns
test_cols = ['GPT-4_turbo_Test_1', 'GPT-4_turbo_Test_2', 'GPT-4_turbo_Test_3',
             'Gemini_Advanced_Test_1', 'Gemini_Advanced_Test_2', 'Gemini_Advanced_Test_3']

# Calculate proportions
proportions = calculate_proportions(df, 'Correct Answer', test_cols)

# Plotting
barWidth = 0.25
r1 = np.arange(len(proportions[0]))
r2 = [x + barWidth for x in r1]

plt.figure(figsize=(8, 4))

# Create bars for correct answers
plt.bar(r1, proportions[0], color='green', width=barWidth, edgecolor='grey', label='Correct')
# Create bars for incorrect answers
plt.bar(r2, proportions[1], color='red', width=barWidth, edgecolor='grey', label='Incorrect')

# Add xticks on the middle of the group bars
plt.xlabel('Tests', fontweight='bold')
plt.xticks([r + barWidth/2 for r in range(len(proportions[0]))],
           ['GPT-4\nTest 1', 'GPT-4\nTest 2', 'GPT-4\nTest 3', 'Gemini\nTest 1', 'Gemini\nTest 2', 'Gemini\nTest 3'])

plt.ylabel('Number of Questions')
plt.title('Proportion of Correct and Incorrect Answers per Test.')
plt.legend()
plt.show()

!pip install ace-tools

# Functions to calculate metrics
def accuracy(correct, test):
    return (correct == test).mean()

def precision(correct, test):
    true_positive = (correct == test).sum()
    false_positive = (correct != test).sum() - test.isna().sum()
    return true_positive / (true_positive + false_positive)

def response_consistency(test1, test2):
    return (test1 == test2).mean()

def consistent_errors(test1, test2):
    errors1 = test1 != df['Correct Answer']
    errors2 = test2 != df['Correct Answer']
    return (errors1 & errors2).mean()

# Initialize results dictionary
results = {
    'Model': [],
    'Metric': [],
    'Test_1': [],
    'Test_2': [],
    'Test_3': [],
    'Mean±SD': []
}

# Models and their tests
models = {
    'GPT-4': ['GPT-4_turbo_Test_1', 'GPT-4_turbo_Test_2', 'GPT-4_turbo_Test_3'],
    'Gemini': ['Gemini_Advanced_Test_1', 'Gemini_Advanced_Test_2', 'Gemini_Advanced_Test_3']
}

# Calculate metrics
for model, tests in models.items():
    acc = [accuracy(df['Correct Answer'], df[test]) for test in tests]
    prec = [precision(df['Correct Answer'], df[test]) for test in tests]
    rc = [response_consistency(df[tests[i]], df[tests[j]]) for i in range(len(tests)) for j in range(i+1, len(tests))]
    ce = [consistent_errors(df[tests[i]], df[tests[j]]) for i in range(len(tests)) for j in range(i+1, len(tests))]

    metrics = [acc, prec, rc, ce]
    metric_names = ['Accuracy', 'Precision', 'Response Consistency', 'Consistent Errors']

    for metric, name in zip(metrics, metric_names):
        results['Model'].append(model)
        results['Metric'].append(name)
        results['Test_1'].append(f"{metric[0]:.2f}")
        results['Test_2'].append(f"{metric[1]:.2f}")
        results['Test_3'].append(f"{metric[2]:.2f}")

        mean_metric = np.mean(metric)
        std_metric = np.std(metric)
        results['Mean±SD'].append(f"{mean_metric:.2f}±{std_metric:.2f}")

# Create results DataFrame
results_df = pd.DataFrame(results)

# Display the results DataFrame
results_df

"""Integrando a nova métrica de avaliação"""

# Functions to calculate metrics
def accuracy(correct, test):
    return (correct == test).mean()

def precision(correct, test):
    true_positive = (correct == test).sum()
    false_positive = (correct != test).sum() - test.isna().sum()
    return true_positive / (true_positive + false_positive)

def response_consistency(test1, test2):
    return (test1 == test2).mean()

def consistent_errors(test1, test2):
    errors1 = test1 != df['Correct Answer']
    errors2 = test2 != df['Correct Answer']
    return (errors1 & errors2).mean()

# Initialize results dictionary
results = {
    'Model': [],
    'Metric': [],
    'Test_1': [],
    'Test_2': [],
    'Test_3': [],
    'Mean±SD': []
}

# Models and their tests
models = {
    'GPT-4': ['GPT-4_turbo_Test_1', 'GPT-4_turbo_Test_2', 'GPT-4_turbo_Test_3'],
    'Gemini': ['Gemini_Advanced_Test_1', 'Gemini_Advanced_Test_2', 'Gemini_Advanced_Test_3']
}

# Weights for MAILLM
alpha = 0.4
beta = 0.3
gamma = 0.2
delta = 0.1

# Calculate metrics
for model, tests in models.items():
    acc = [accuracy(df['Correct Answer'], df[test]) for test in tests]
    prec = [precision(df['Correct Answer'], df[test]) for test in tests]
    rc = [response_consistency(df[tests[i]], df[tests[j]]) for i in range(len(tests)) for j in range(i+1, len(tests))]
    ce = [consistent_errors(df[tests[i]], df[tests[j]]) for i in range(len(tests)) for j in range(i+1, len(tests))]

    metrics = [acc, prec, rc, ce]
    metric_names = ['Accuracy', 'Precision', 'Response Consistency', 'Consistent Errors']

    # Calculate MAILLM for each test
    maillm = [alpha * a + beta * p + gamma * r - delta * c for a, p, r, c in zip(acc, prec, rc[:3], ce[:3])]

    for metric, name in zip(metrics, metric_names):
        results['Model'].append(model)
        results['Metric'].append(name)
        results['Test_1'].append(f"{metric[0]:.2f}")
        results['Test_2'].append(f"{metric[1]:.2f}")
        results['Test_3'].append(f"{metric[2]:.2f}")

        mean_metric = np.mean(metric)
        std_metric = np.std(metric)
        results['Mean±SD'].append(f"{mean_metric:.2f}±{std_metric:.2f}")

    # Add MAILLM to the results
    results['Model'].append(model)
    results['Metric'].append('MAILLM')
    results['Test_1'].append(f"{maillm[0]:.2f}")
    results['Test_2'].append(f"{maillm[1]:.2f}")
    results['Test_3'].append(f"{maillm[2]:.2f}")

    mean_maillm = np.mean(maillm)
    std_maillm = np.std(maillm)
    results['Mean±SD'].append(f"{mean_maillm:.2f}±{std_maillm:.2f}")

# Create results DataFrame
results_df = pd.DataFrame(results)

# Display the results DataFrame
results_df

"""Plotando em gráficos"""

# Function to create boxplots for each metric
def create_boxplot(df, metric):
    # Filter data for the given metric
    metric_data = df[df['Metric'] == metric]
    data = [metric_data['Test_1'].astype(float), metric_data['Test_2'].astype(float), metric_data['Test_3'].astype(float)]

    # Create boxplot
    plt.figure(figsize=(4, 2))
    plt.boxplot(data, labels=['Test_1', 'Test_2', 'Test_3'])
    plt.title(f'Boxplot for {metric}')
    plt.ylabel('Value')
    plt.xlabel('Tests')
    plt.grid(True)
    plt.show()

# List of metrics
metrics = ['Accuracy', 'Precision', 'Response Consistency', 'Consistent Errors', 'MAILLM']

# Create boxplots for each metric
for metric in metrics:
    create_boxplot(results_df, metric)



df = results_df
# Function to create boxplots for each metric with both models in a single plot
def create_combined_boxplot(df, metric):
    # Filter data for the given metric and models
    metric_data_gpt4 = df[(df['Metric'] == metric) & (df['Model'] == 'GPT-4')]
    metric_data_gemini = df[(df['Metric'] == metric) & (df['Model'] == 'Gemini')]

    # Prepare data for boxplot
    data = [
        metric_data_gpt4['Test_1'].astype(float).values.tolist() +
        metric_data_gpt4['Test_2'].astype(float).values.tolist() +
        metric_data_gpt4['Test_3'].astype(float).values.tolist(),

        metric_data_gemini['Test_1'].astype(float).values.tolist() +
        metric_data_gemini['Test_2'].astype(float).values.tolist() +
        metric_data_gemini['Test_3'].astype(float).values.tolist()
    ]

    # Create boxplot
    plt.figure(figsize=(4, 2))
    plt.boxplot(data, labels=['GPT-4', 'Gemini'])
    plt.title(f'Boxplot for {metric}', fontsize=12)
    plt.ylabel('Value', fontsize=12)
    plt.xlabel('Models', fontsize=12)
    plt.grid(False)
    plt.show()

# List of metrics
metrics = ['Accuracy', 'Precision', 'Response Consistency', 'Consistent Errors', 'MAILLM']

# Create combined boxplots for each metric
for metric in metrics:
    create_combined_boxplot(results_df, metric)

